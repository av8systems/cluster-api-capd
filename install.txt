##################################################################################
Use the Cluster API CAPD provider to provision and bootstrap a Kubernetes cluster.
This guide also addresses some of problems you might run into (that make you want to give up or put it off) 
after creating you managment or workload cluster.
##################################################################################                        

#########
IMPORTANT
#########
# Anytime you see  <<EOF copy the full block of text until you see EOF at the bottom of the block
# As an example copy the full block below from line 12 to 17
sudo tee /etc/security/limits.d/99-nofile.conf >/dev/null <<'EOF'
* soft nofile 1048576
* hard nofile 1048576
root soft nofile 1048576
root hard nofile 1048576
EOF

# Formatting might be off and must be fixed if pasting from here into vim editor
# As a workaround you can create the files using notepad or vs code
# As an example for the management cluster
# Create a new file then paste in the yaml from this document
# then save it in the '/mnt/c/av8systems/cluster-api/tmp' directory with the name 'd01capimgmt001.yaml' 

#####################
Start of Lab Creation
#####################
# Install wsl
# List currently installed wsl distributions
wsl --list --verbose

# List available wsl distributions 
wsl --list --online

# Install a wsl distribution
wsl --install -d Ubuntu-22.04

# Create Environment Variables
export MANAGEMENT_CLUSTER_NAME='d01capimgmt001' 
export WORKLOAD_CLUSTER_NAME='d01av8test001'
export WORKLOAD_KUBECONFIG_DIR="/mnt/c/av8systems/cluster-api/providers/capd/clusters/workload/${WORKLOAD_CLUSTER_NAME}/configs"
export WORKLOAD_KUBECONFIG_FILE="${WORKLOAD_KUBECONFIG_DIR}/${WORKLOAD_CLUSTER_NAME}.kubeconfig"
export CLUSTER_TOPOLOGY=true
export CLUSTERCTL_DEFAULT_INFRASTRUCTURE=docker

# Create directories
mkdir /mnt/c/av8systems/cluster-api/providers/capd/clusters/management -p
mkdir /mnt/c/av8systems/cluster-api/providers/capd/clusters/workload -p
mkdir /mnt/c/av8systems/cluster-api/tools -p
mkdir /mnt/c/av8systems/cluster-api/apps -p
mkdir /mnt/c/av8systems/cluster-api/tmp -p
mkdir $WORKLOAD_KUBECONFIG_DIR  -p 

# Cluster API directory structure
cluster-api
  L providers
    L capd
      L clusters
        L management
          L kind-capi-mgmt.yaml 
        L workload
          L clusterName
           L clusterName-cluster.yaml
           L clusterName.kubeconfig
           L clusterName-metallb-config.yaml

# Move into the tmp directory
cd /mnt/c/av8systems/cluster-api/tmp

# Install prerequisite packages
sudo apt-get update
sudo apt-get install -y ca-certificates curl gnupg apt-transport-https lsb-release git jq --yes 

# Install docker engine
sudo apt-get install -y docker.io
sudo usermod -aG docker $USER
newgrp docker
docker version

###################################
Install required command line tools
###################################
# kubectl
curl -LO "https://dl.k8s.io/release/$(curl -Ls https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
kubectl version --client

# kind
curl -Lo ./kind https://kind.sigs.k8s.io/dl/latest/kind-linux-amd64
chmod +x ./kind
sudo mv ./kind /usr/local/bin/kind
kind version

# clusterctl
curl -L https://github.com/kubernetes-sigs/cluster-api/releases/latest/download/clusterctl-linux-amd64 -o clusterctl
chmod +x clusterctl
sudo mv clusterctl /usr/local/bin/clusterctl
clusterctl version

# helm
curl -fsSL https://packages.buildkite.com/helm-linux/helm-debian/gpgkey | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
echo "deb [signed-by=/usr/share/keyrings/helm.gpg] https://packages.buildkite.com/helm-linux/helm-debian/any/ any main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
sudo apt-get update
sudo apt-get install helm 

# istioctl
curl -L https://istio.io/downloadIstio | sh -
cd istio-*
sudo mv bin/istioctl /usr/local/bin/
istioctl version --remote=false

# Move back to tmp directory
cd /mnt/c/av8systems/cluster-api/tmp

# argocd
curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64
sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd
argocd version

# Vault
curl -fsSL  https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(grep -oP '(?<=UBUNTU_CODENAME=).*' /etc/os-release || lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/hashicorp.list
sudo apt update && sudo apt install vault
vault version

#############################
Configure Linux host settings
#############################
# Load br_netfilter kernel module
sudo modprobe br_netfilter || true

# Load overlay kernel module 
sudo modprobe overlay || true

# Configure network settings
cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward=1
EOF

# Verify net bridge configurations (optional)
sysctl net.bridge.bridge-nf-call-iptables net.ipv4.ip_forward

# Update ulimits for local host
sudo tee /etc/security/limits.d/99-nofile.conf >/dev/null <<'EOF'
* soft nofile 1048576
* hard nofile 1048576
root soft nofile 1048576
root hard nofile 1048576
EOF

# Update ulimits for docker daemon
sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json >/dev/null <<'EOF'
{
  "default-ulimits": {
    "nofile": { "Name": "nofile", "Hard": 1048576, "Soft": 1048576 }
  }
}
EOF

# Update inotifty settings
sudo tee /etc/sysctl.d/99-inotify-k8s.conf >/dev/null <<'EOF'
fs.inotify.max_user_instances=1024
fs.inotify.max_user_watches=1048576
fs.inotify.max_queued_events=32768
EOF

# Reload kernel parameters
sudo sysctl --system

####################
Infrastructure Tasks
####################
# Create management cluster yaml 
vim ${MANAGEMENT_CLUSTER_NAME}.yaml

# press the "i" key to insert text

# Copy and paste below yaml into text editor (can also use notepad or vs code)
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
    image: kindest/node:v1.33.1@sha256:8d866994839cd096b3590681c55a6fa4a071fdaf33be7b9660e5697d2ed13002
    extraMounts:
      - hostPath: /var/run/docker.sock
        containerPath: /var/run/docker.sock

# press "ctl + c" to exit out of insert mode
# press "shift + :" to give it an exit command
# type "wq" then press enter to save the file

# Create kind management cluster from yaml config file
kind create cluster --name ${MANAGEMENT_CLUSTER_NAME} --config ${MANAGEMENT_CLUSTER_NAME}.yaml

# Wait for the management cluster node status to be ready
kubectl get node --watch

# Initialize management cluster
clusterctl init --infrastructure docker

# Ensure all pods reach a running status
kubectl get pods -A --watch

# Generate yaml for workload cluster
clusterctl generate cluster ${WORKLOAD_CLUSTER_NAME} --flavor development --infrastructure docker --kubernetes-version v1.32.0 --control-plane-machine-count=1 --worker-machine-count=2 > ${WORKLOAD_CLUSTER_NAME}.yaml

# Create workload cluster
kubectl apply -f ${WORKLOAD_CLUSTER_NAME}.yaml

# Wait for workload cluster nodes to be created
# CP UP-TO-DATE should say 1
# W UP-TO-DATE should say 2 (or the number of desired worker nodes)
kubectl get cluster ${WORKLOAD_CLUSTER_NAME} --watch

# Export workload cluster kubeconfig file
clusterctl get kubeconfig "$WORKLOAD_CLUSTER_NAME" > "$WORKLOAD_KUBECONFIG_FILE"

# Merge workload cluster kubeconfig file with the main config file (copy and paste both lines)
KUBECONFIG="$HOME/.kube/config:$WORKLOAD_KUBECONFIG_FILE" \
kubectl config view --merge --flatten > "$HOME/.kube/config.merged"

# Make a backup the current config file
cp "$HOME/.kube/config" "$HOME/.kube/config.bak.$(date +%Y%m%d%H%M%S)"

# Replace the current config file with the merged config file
mv "$HOME/.kube/config.merged" "$HOME/.kube/config"

# Set kubectl context to your workload cluster
kubectl config use-context ${WORKLOAD_CLUSTER_NAME}-admin@${WORKLOAD_CLUSTER_NAME}

# Run kubectl command to ensure you in the context of the correct cluster 
# Nodes will have a not ready status
kubectl get nodes

# Install calico cni on the workload cluster
# Nodes will become ready after calico install
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.3/manifests/calico.yaml

# Wait for all pods to have a status of running
kubectl get pods -A --watch

# Install metallb load balancer
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.14.8/config/manifests/metallb-native.yaml

# Get docker network CIDR your workload nodes are on
docker inspect ${WORKLOAD_CLUSTER_NAME}-lb --format '{{range $k,$v := .NetworkSettings.Networks}}{{$k}} {{$v.IPAddress}} {{$v.NetworkID}}{{end}}'

# Get subnet for workload network
NET_NAME=$(docker inspect ${WORKLOAD_CLUSTER_NAME}-lb --format '{{range $k,$v := .NetworkSettings.Networks}}{{$k}}{{end}}')
docker network inspect "$NET_NAME" --format '{{json .IPAM.Config}}' | jq

# Create metallb IP Pool (with small range from workload network) and advertisement yaml file
# Replace the ip range in the yaml file
vim ${WORKLOAD_CLUSTER_NAME}-metallb-ipam.yaml
# copy yaml below into vim editor after you add the correct Ip range
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: capd-pool
  namespace: metallb-system
spec:
  addresses:
  - 172.18.255.200-172.18.255.250
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: capd-l2
  namespace: metallb-system
spec:
  ipAddressPools:
  - capd-pool

# Apply metallb IP and Advertisement yaml file
kubectl apply -f ${WORKLOAD_CLUSTER_NAME}-metallb-ipam.yaml

# Install local Path Provisioner 
kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml

# Set the local path provisioner storage class as the default
kubectl patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}' 

# Install the metrics server
helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
helm repo update metrics-server
# run next two lines together
helm upgrade --install metrics-server metrics-server/metrics-server -n kube-system \
--set 'args={--kubelet-insecure-tls,--kubelet-preferred-address-types=InternalIP\,ExternalIP\,Hostname}'

# Install vertical pod autoscaler
# make sure you are in the /tmp directory
git clone https://github.com/kubernetes/autoscaler.git
cd autoscaler/vertical-pod-autoscaler/
./hack/vpa-up.sh

# Move back to tmp directory
cd /mnt/c/av8systems/cluster-api/tmp

###############################
Install Observability workloads
###############################
# Install Istio
istioctl install -y --set profile=demo

# Move to istio addons directory
# istio directory name may be different depending on the version downloaded
cd istio-1.28.2/samples/addons

# Install  Kiali
kubectl apply -f ./kiali.yaml

# Install  Promethus
kubectl apply -f ./prometheus.yaml

# Install Grafana
kubectl apply -f ./grafana.yaml

# Install Jaeger
kubectl apply -f ./jaeger.yaml

# Install Loki
kubectl apply -f ./loki.yaml

# Move back to tmp directory
cd /mnt/c/av8systems/cluster-api/tmp

# Label all new namespaces with pod security configuration
kubectl label --overwrite ns --all pod-security.kubernetes.io/enforce=privileged pod-security.kubernetes.io/audit=baseline pod-security.kubernetes.io/warn=baseline

##########################
Install Security Workloads
##########################
# Install Falco and Falco UI
helm repo add falcosecurity https://falcosecurity.github.io/charts
helm repo update falcosecurity
helm install falco falcosecurity/falco --create-namespace --namespace falco --set falcosidekick.enabled=true --set falcosidekick.webui.enabled=true

# Install Kyverno
helm repo add kyverno https://kyverno.github.io/kyverno/
helm repo update kyverno
helm install kyverno kyverno/kyverno -n kyverno --create-namespace

# Install Kyverno Policy Reporter with UI
helm repo add policy-reporter https://kyverno.github.io/policy-reporter
helm repo update policy-reporter
helm install policy-reporter policy-reporter/policy-reporter --create-namespace -n policy-reporter --set ui.enabled=true

# Install Vault Server 
helm repo add hashicorp https://helm.releases.hashicorp.com
helm repo update hashicorp
helm install vault hashicorp/vault -n vault --create-namespace

# Install Vault Secrets Operator
helm repo add hashicorp https://helm.releases.hashicorp.com
helm install vault-secrets-operator hashicorp/vault-secrets-operator -n vault-secrets-operator-system --create-namespace

# Install Trivy
helm repo add aqua https://aquasecurity.github.io/helm-charts/
helm repo update aqua
helm install trivy-operator aqua/trivy-operator --namespace trivy-system --create-namespace --set="trivy.ignoreUnfixed=true"

# Label all new namespaces with pod security configuration
kubectl label --overwrite ns --all pod-security.kubernetes.io/enforce=privileged pod-security.kubernetes.io/audit=baseline pod-security.kubernetes.io/warn=baseline

#######################################################
# Install Application Deployment & Management Workloads
#######################################################
# Install Argo CD
helm repo add argo https://argoproj.github.io/argo-helm/
helm repo update argo
helm install argocd argo/argo-cd --namespace argocd --create-namespace

# Install Keda
helm repo add kedacore https://kedacore.github.io/charts  
helm repo update kedacore
helm install keda kedacore/keda --namespace keda --create-namespace

# Install Harbor 
helm repo add harbor https://helm.goharbor.io
helm repo update harbor
helm install harbor harbor/harbor --namespace harbor --create-namespace

# Label all new namespaces with pod security configuration
kubectl label --overwrite ns --all pod-security.kubernetes.io/enforce=privileged pod-security.kubernetes.io/audit=baseline pod-security.kubernetes.io/warn=baseline

########################
Destroy Lab environment
########################
# Set context to management cluster
kubectl config use-context kind-${MANAGEMENT_CLUSTER_NAME}

# Delete the workload cluster
kubectl delete cluster ${WORKLOAD_CLUSTER_NAME}

# Delete the management cluster
sudo kind delete cluster --name ${MANAGEMENT_CLUSTER_NAME}

# Remove directories
rm /mnt/c/av8systems -fr

# Completely uninstall wsl distribution
wsl --unregister Ubuntu-22.04 

#############
Helpful Links
#############
# Install WSL
https://learn.microsoft.com/en-us/windows/wsl/install

# Install Kubectl
https://kubernetes.io/docs/tasks/tools/install-kubectl-linux

# Install Kind
https://kind.sigs.k8s.io/docs/user/quick-start/#installation

# Install Clusterctl
https://cluster-api.sigs.k8s.io/user/quick-start#install-clusterctl

# Install Vault Cli
https://developer.hashicorp.com/vault/install#linux

# Install Helm
https://helm.sh/docs/intro/install/

# Install Istioctl
https://istio.io/latest/docs/setup/additional-setup/download-istio-release

# Install ArgoCD Commandline Tool
https://argo-cd.readthedocs.io/en/stable/cli_installation

# ClustAPI Troubleshooting Guide
https://cluster-api.sigs.k8s.io/user/troubleshooting

# Install Falco
https://falco.org/docs/setup/kubernetes/

# Install Vault and Vault Secrets Operator Helm charts
https://developer.hashicorp.com/vault/tutorials/kubernetes-introduction/vault-secrets-operator

# Install Kyverno
https://kyverno.io/docs/installation/methods/

# Install Kyerno Policy Reporter
https://kyverno.github.io/policy-reporter-docs/getting-started/installation.html

# Install Trivy Helm Chart
https://aquasecurity.github.io/trivy-operator/v0.1.5/operator/installation/helm/

# Install Metrics Server Helm Chart
https://artifacthub.io/packages/helm/metrics-server/metrics-server

# Install Vertical Pod Autoscaler
https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/docs/installation.md

# Install Keda
https://keda.sh/docs/2.18/deploy/

# Install Harbor
https://goharbor.io/docs/edge/install-config/harbor-ha-helm/
